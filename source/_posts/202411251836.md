---
title: Spark Sql 常用函数 (Scala 语言)
date: 2024-11-25 18:36:22
tags:
    - Spark
    - Scala
cover: /assets/images/post/spark.jpg
---
基于 org.apache.spark.sql.functions._ 包下的函数

### col
用于在DataFrame或Dataset中引用列，通常与Spark的SQL查询或者DataFrame操作一起使用。
语法：
```scala
col(colName: String): Column
```
### column
与 col 函数功能相同。
### lit
用于将一个常量值转换为 Column 类型，以便在 Spark SQL 操作中使用。
```scala
lit(literal: Any): Column
```
### typeLit
typedLit 函数是一个类型安全的版本的 lit 函数。它用于将字面量（常量）转换为指定类型的 Column，并确保在编译时类型匹配。typedLit 适用于 Spark 3.x 版本，提供比 lit 更严格的类型检查，从而避免潜在的类型不匹配问题。
```scala
typedLit[T: TypeTag](literal: T): Column
```
### greatest
用于比较多列的值，返回每行中这些列中的最大值。
```scala
greatest(exprs: Column*): Column
greatest(columnName: String, columnNames: String*): Column
```
### current_date
返回当前的日期（以 yyyy-MM-dd 格式表示）。
```scala
current_date(): Column
```
### date_sub
用于从指定的日期中减去给定的天数，并返回新的日期。
```scala
date_sub(start: Column, days: Int): Column
```
### date_format
用于将日期类型的数据格式化为指定的字符串格式。
```scala
date_format(dateExpr: Column, format: String): Column
```
### when
条件表达式，通常与 otherwise() 函数一起使用，用于根据条件返回不同的值。
```scala
when(condition: Column, value: Any): Column
    .otherwise(value: Any): Column
```
### max
它通常用于对数值型数据进行分组聚合，或者在没有分组时返回整个列的最大值。
```scala
max(e: Column): Column
```
### to_timestamp
将一个字符串或者日期列转换为 timestamp 类型。
```scala
to_timestamp(s: Column): Column
to_timestamp(s: Column, fmt: String): Column
```
### row_number
用于为每个分组中的行分配一个唯一的递增整数值。它通常与窗口函数一起使用，以实现更复杂的分组和排序操作。
以下示例是基于 id 列进行分组，并按照 time 列进行降序排序，为每个分组中的行分配一个唯一的行号。
```scala
row_number()
    .over(
        Window
            .partitionBy("id")
            .orderBy(desc("time"))
    )
```
